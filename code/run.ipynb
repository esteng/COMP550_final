{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def process_csv(path):\n",
    "    print(\"in process\")\n",
    "    with open(path) as f1:\n",
    "        reader = csv.DictReader(f1)\n",
    "        sent = None\n",
    "        to_ret = []\n",
    "        # add sentence info\n",
    "        for i, row in enumerate(reader):\n",
    "            if row[\"Sentence #\"] is not None:\n",
    "                sent = row[\"Sentence #\"]\n",
    "            else:\n",
    "                row[\"Sentence #\"] = sent\n",
    "\n",
    "            to_ret.append(row)\n",
    "        return to_ret\n",
    "\n",
    "def get_all_sentences(data):\n",
    "    all_sents = []\n",
    "    sentence = []\n",
    "    current_sent = data[0][\"Sentence #\"]\n",
    "    for i, row in enumerate(data): \n",
    "        if current_sent == row[\"Sentence #\"]:\n",
    "            sentence.append((row[\"Word\"], row[\"Tag\"]))\n",
    "        else:\n",
    "            all_sents.append(sentence)\n",
    "            sentence = [(row[\"Word\"], row[\"Tag\"])]\n",
    "            current_sent = row[\"Sentence #\"]\n",
    "    return all_sents\n",
    "\n",
    "def make_x_y(data, size):\n",
    "    all_sentences = get_all_sentences(data)\n",
    "\n",
    "    all_sent_len = [len(x) for x in all_sentences]\n",
    "    just_sents = [[y[0] for y in x] for x in all_sentences]\n",
    "    max_len = max(all_sent_len)\n",
    "\n",
    "    model = gensim.models.Word2Vec(just_sents, min_count=1, size = size)\n",
    "\n",
    "    all_tags = sorted(list(set([row[\"Tag\"] for row in data])))\n",
    "    tag_to_one_hot = {}\n",
    "    one_hot_to_tag = {}\n",
    "    for i, tag in enumerate(all_tags):\n",
    "        tag_vector = np.zeros((len(all_tags)))\n",
    "        tag_vector[i] = 1\n",
    "        tag_to_one_hot[tag] = tag_vector\n",
    "        one_hot_to_tag[tuple(tag_vector)] = tag\n",
    "\n",
    "    null_embedding = np.zeros((size))\n",
    "    null_tag = np.zeros((len(tag_vector)))\n",
    "\n",
    "    x_full = []\n",
    "    y_full = []\n",
    "\n",
    "    for sent in all_sentences:\n",
    "        w2v_seq = []\n",
    "        tag_seq = []\n",
    "        for i, tup in enumerate(sent):\n",
    "            word, tag = tup\n",
    "            w2v_seq.append(model[word])\n",
    "            tag_seq.append(tag_to_one_hot[tag])\n",
    "        for j in range(i, max_len):\n",
    "            w2v_seq.append(null_embedding)\n",
    "            tag_seq.append(null_tag)\n",
    "        x_full.append(w2v_seq)\n",
    "        y_full.append(tag_seq)\n",
    "\n",
    "    return x_full, y_full, model, (tag_to_one_hot, one_hot_to_tag)\n",
    "\n",
    "\n",
    "\n",
    "# data = process_csv(\"../data/entity-annotated-corpus/ner_dataset.csv\")\n",
    "# X, Y, word_embeddings, tag_embeddings = make_x_y(data, 300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "in process\n",
      "getting X Y sets\n",
      "300\n",
      "defining model...\n",
      "training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-435771130e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1379\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    102\u001b[0m                             \u001b[0;34m': data should be a Numpy array, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                             \u001b[0;34m'or list/dict of Numpy arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                             'Found: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# Case: model expects multiple inputs but only received\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36marray_repr\u001b[0;34m(arr, max_line_width, precision, suppress_small)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         lst = array2string(arr, max_line_width, precision, suppress_small,\n\u001b[0;32m-> 1898\u001b[0;31m                            ', ', \"array(\")\n\u001b[0m\u001b[1;32m   1899\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# show zero-length shape unless it is (0,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[], shape=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/numpy/core/arrayprint.pyc\u001b[0m in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         lst = _array2string(a, max_line_width, precision, suppress_small,\n\u001b[0;32m--> 463\u001b[0;31m                             separator, prefix, formatter=formatter)\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/numpy/core/arrayprint.pyc\u001b[0m in \u001b[0;36m_array2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, formatter)\u001b[0m\n\u001b[1;32m    334\u001b[0m     lst = _formatArray(a, format_function, len(a.shape), max_line_width,\n\u001b[1;32m    335\u001b[0m                        \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                        _summaryEdgeItems, summary_insert)[:-1]\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/numpy/core/arrayprint.pyc\u001b[0m in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, rank, max_line_len, next_line_prefix, separator, edge_items, summary_insert)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrailing_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extendLine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_line_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elias/anaconda/envs/nlp/lib/python2.7/site-packages/numpy/core/arrayprint.pyc\u001b[0m in \u001b[0;36m_extendLine\u001b[0;34m(s, line, word, max_line_len, next_line_prefix)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_extendLine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_line_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_line_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "# from data import process_csv, make_x_y\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "def define_model(embedding_size, tag_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(104,300)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(tag_size, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"loading data\")\n",
    "data = process_csv(\"../data/entity-annotated-corpus/ner_dataset.csv\")\n",
    "print(\"getting X Y sets\")\n",
    "X, Y, word_embeddings, tag_embeddings = make_x_y(data, 300)\n",
    "\n",
    "train, test, dev = .7, .2, .1\n",
    "\n",
    "train_split = int(len(X)*train)\n",
    "test_split = train_split+1+int(math.floor(len(X)*test))\n",
    "\n",
    "\n",
    "XY = zip(X, Y)\n",
    "np.random.shuffle(XY)\n",
    "\n",
    "X, Y = zip(*XY)\n",
    "X_train, Y_train = X[0:train_split], Y[0:train_split]\n",
    "X_test, Y_test = X[train_split +1 : test_split], Y[train_split + 1:test_split]\n",
    "X_dev, Y_dev = X[test_split +1 :], Y[test_split +1 :]\n",
    "\n",
    "\n",
    "tag_to_vec, vec_to_tag = tag_embeddings\n",
    "print(word_embeddings.vector_size)\n",
    "\n",
    "tag_length = len(tag_to_vec.values())\n",
    "print(\"defining model...\")\n",
    "model = define_model(word_embeddings.vector_size, tag_length)\n",
    "print(\"training...\")\n",
    "model.fit(X_dev, Y_dev, epochs=10, batch_size=32)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
